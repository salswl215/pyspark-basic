{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132d2831-74cb-4125-bca3-e143b6de2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    functions as f,\n",
    "    SparkSession,\n",
    "    types as t\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_join\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc82c5e2-e86d-4b4e-8332-a88f9c818595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------+\n",
      "|  id|         name| company|\n",
      "+----+-------------+--------+\n",
      "|1000|Neville Hardy|   Apple|\n",
      "|2000|  Dacia Cohen|Alphabet|\n",
      "|3000|    Elois Cox|  Neflix|\n",
      "|4000| Junita Meyer|    Meta|\n",
      "|5000| Cleora Banks|  Amazon|\n",
      "+----+-------------+--------+\n",
      "\n",
      "+----+------+--------------+\n",
      "|  id|salary|    department|\n",
      "+----+------+--------------+\n",
      "|1000|150000|      engineer|\n",
      "|2000|240000|       manager|\n",
      "|3000|120000|human resource|\n",
      "|6000|100000|         sales|\n",
      "+----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user data\n",
    "user_data = [\n",
    "    [\"1000\", \"Neville Hardy\", \"Apple\"],\n",
    "    [\"2000\", \"Dacia Cohen\", \"Alphabet\"],\n",
    "    [\"3000\", \"Elois Cox\", \"Neflix\"],\n",
    "    [\"4000\", \"Junita Meyer\", \"Meta\"],\n",
    "    [\"5000\", \"Cleora Banks\", \"Amazon\"]]\n",
    "\n",
    "user_col = ['id', 'name', 'company']\n",
    "df_user = spark.createDataFrame(data=user_data, schema=user_col)\n",
    "df_user.show()\n",
    "\n",
    "# salary data\n",
    "salary_data = [\n",
    "    [\"1000\", \"150000\", \"engineer\"],\n",
    "    [\"2000\", \"240000\", \"manager\"],\n",
    "    [\"3000\", \"120000\", \"human resource\"],\n",
    "    [\"6000\", \"100000\", \"sales\"]]\n",
    "\n",
    "salary_col = ['id', 'salary', 'department']\n",
    "df_salary = spark.createDataFrame(data=salary_data, schema=salary_col)\n",
    "df_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccbe55-6725-4b5b-b9e6-c5f63d06f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inner join: join the two dataframes on common key columns.\n",
    "# # dataframe1.join(dataframe2,dataframe1.column_name ==  dataframe2.column_name,”inner”)\n",
    "print(\"== inner join ==\")\n",
    "df_user.join(df_salary,\n",
    "               df_user.id == df_salary.id,\n",
    "               \"inner\").show()\n",
    "\n",
    "# # inner join, then filter\n",
    "df_user.join(df_salary,\n",
    "               df_user.id == df_salary.id,\n",
    "               \"inner\").filter(df_user.id == 1000).show()\n",
    "\n",
    "# # inner join, then where\n",
    "df_user.join(df_salary,\n",
    "               df_user.id == df_salary.id,\n",
    "               \"inner\").where(df_user.id == 1000).show()\n",
    "\n",
    "# # multiple join with &\n",
    "df_user.join(df_salary,\n",
    "               (df_user.id == df_salary.id) & (df_user.id == 1000)\n",
    "            ).show()\n",
    "\n",
    "# # full outer join: join the two dataframes with all matching and non-matching rows\n",
    "print(\"== full outer join ==\")\n",
    "df_user.join(df_salary, \n",
    "               df_user.id == df_salary.id, \n",
    "               \"fullouter\").show()\n",
    "\n",
    "# # left join:  joins by returning all rows from the first dataframe and only matched rows from the second one\n",
    "print(\"== left join ==\")\n",
    "df_user.join(df_salary, \n",
    "               df_user.id == df_salary.id, \n",
    "               \"left\").show()\n",
    "\n",
    "# # right join: joins by returning all rows from the second dataframe and only matched rows from the first one\n",
    "print(\"== right join ==\")\n",
    "df_user.join(df_salary, \n",
    "               df_user.id == df_salary.id, \n",
    "               \"right\").show()\n",
    "\n",
    "# # left semi join: join all rows from the first dataframe and return only matched rows from the second one\n",
    "print(\"== left semi join ==\")\n",
    "df_user.join(df_salary, \n",
    "               df_user.id == df_salary.id, \n",
    "               \"leftsemi\").show()\n",
    "\n",
    "# # left anti join: join returns only columns from the first dataframe for non-matched records of the second dataframe\n",
    "print(\"== left anti join ==\")\n",
    "df_user.join(df_salary, \n",
    "               df_user.id == df_salary.id, \n",
    "               \"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23e93d-0d5f-4245-a18c-3349ab05cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SQL join\n",
    "df_user.createOrReplaceTempView(\"user\")\n",
    "df_salary.createOrReplaceTempView(\"salary\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM user, salary WHERE user.id == salary.id\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM user INNER JOIN salary ON user.id == salary.id\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
